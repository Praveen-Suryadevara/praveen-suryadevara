{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e450991",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c1d5a2",
   "metadata": {},
   "source": [
    "## Importing Libraries and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a912f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speaker</th>\n",
       "      <th>position</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Albania</td>\n",
       "      <td>Mr. NAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33: May I first convey to our President the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>ARG</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Mr. DE PABLO PARDO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>177.\\t : It is a fortunate coincidence that pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Mr. McMAHON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.\\t  It is a pleasure for me to extend to y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>AUT</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Mr. KIRCHSCHLAEGER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155.\\t  May I begin by expressing to Ambassado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>BEL</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Mr. HARMEL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>176. No doubt each of us, before coming up to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>BLR</td>\n",
       "      <td>Belarus</td>\n",
       "      <td>Mr. GURINOVICH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n71.\\t. We are today mourning the untimely de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>BOL</td>\n",
       "      <td>Bolivia, Plurinational State of</td>\n",
       "      <td>Mr. CAMACHO OMISTE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.\\t  I wish to congratulate the President o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>BRA</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Mr. GIBSON BARBOZA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.\\tMr. President, I should like, first of all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Mr. SHARP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nThe General Assembly is fortunate indeed to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>CMR</td>\n",
       "      <td>Cameroon</td>\n",
       "      <td>Mr. AHIDJO</td>\n",
       "      <td>President</td>\n",
       "      <td>: A year ago I came here as the Acting Preside...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session  year country                     country_name             speaker  \\\n",
       "0       25  1970     ALB                          Albania             Mr. NAS   \n",
       "1       25  1970     ARG                        Argentina  Mr. DE PABLO PARDO   \n",
       "2       25  1970     AUS                        Australia         Mr. McMAHON   \n",
       "3       25  1970     AUT                          Austria  Mr. KIRCHSCHLAEGER   \n",
       "4       25  1970     BEL                          Belgium          Mr. HARMEL   \n",
       "5       25  1970     BLR                          Belarus      Mr. GURINOVICH   \n",
       "6       25  1970     BOL  Bolivia, Plurinational State of  Mr. CAMACHO OMISTE   \n",
       "7       25  1970     BRA                           Brazil  Mr. GIBSON BARBOZA   \n",
       "8       25  1970     CAN                           Canada           Mr. SHARP   \n",
       "9       25  1970     CMR                         Cameroon          Mr. AHIDJO   \n",
       "\n",
       "     position                                               text  \n",
       "0         NaN  33: May I first convey to our President the co...  \n",
       "1         NaN  177.\\t : It is a fortunate coincidence that pr...  \n",
       "2         NaN  100.\\t  It is a pleasure for me to extend to y...  \n",
       "3         NaN  155.\\t  May I begin by expressing to Ambassado...  \n",
       "4         NaN  176. No doubt each of us, before coming up to ...  \n",
       "5         NaN  \\n71.\\t. We are today mourning the untimely de...  \n",
       "6         NaN  135.\\t  I wish to congratulate the President o...  \n",
       "7         NaN  1.\\tMr. President, I should like, first of all...  \n",
       "8         NaN  \\nThe General Assembly is fortunate indeed to ...  \n",
       "9  President   : A year ago I came here as the Acting Preside...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data set using pandas.\n",
    "import pandas as pd\n",
    "\n",
    "file= \"C:/Users/prave/Downloads/un-general-debates-blueprint.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e3474c",
   "metadata": {},
   "source": [
    "## Looking at Couple of Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a879ce33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\ufeffIt is indeed a pleasure for me and the members of my delegation to extend to Ambassador Garba our sincere congratulations on his election to the presidency of the forty-fourth session of the General Assembly. His election to this high office is a well-deserved tribute to his personal qualities and experience. I am fully confident that under his able and wise leadership the Assembly will further c'\n",
      "'\\ufeffI wish to join\\nother representatives in congratulating you, Sir, on\\nyour unanimous election as President of the fifty-sixth\\nsession of the General Assembly. We are confident that\\n27\\n\\nunder your able guidance the work of this General\\nAssembly session will be another milestone on the new\\ninternational scene, particularly in confronting the new\\nchallenges facing our world, especially after the\\nextre'\n"
     ]
    }
   ],
   "source": [
    "# Printing a slice of text from row 2666, specifically the first 400 characters\n",
    "print(repr(df.iloc[2666][\"text\"][0:400]))\n",
    "print(repr(df.iloc[4726][\"text\"][0:400]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365ecf9",
   "metadata": {},
   "source": [
    "## Let's Split Speech Into Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec890f88",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Importing the regular expression\n",
    "import re\n",
    "# Splitting the text in the \"text\" column\n",
    "df[\"paragraphs\"] = df[\"text\"].map(lambda text: re.split(r'\\.\\s*\\n', text))\n",
    "# Calculating the number of paragraphs for each row and storing it in a new column\n",
    "df[\"number_of_paragraphs\"] = df[\"paragraphs\"].map(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36b115",
   "metadata": {},
   "source": [
    "## Start by Getting TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2298d1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7507, 24604)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the shape of the TF-IDF matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stopwords\n",
    "\n",
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(df['text'])\n",
    "tfidf_text_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fed9b4",
   "metadata": {},
   "source": [
    "## Making Data Frame of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf2699fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33: May I first convey to our President the co...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35.\\tThe utilization of the United Nations to ...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.\\tThe whole of progressive mankind recalls ...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.\\tAll this has had well known consequences ...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.\\tOne of the undeniable proofs that the Uni...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39.\\tUndoubtedly, such a state of affairs in t...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40.\\tThe liberation movement at the world leve...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41.\\tPanic-stricken at the impetuous growth of...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42.\\tAlthough split by numerous contradictions...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>43.\\tIn that connexion we can cite, simultaneo...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  year\n",
       "0  33: May I first convey to our President the co...  1970\n",
       "1  35.\\tThe utilization of the United Nations to ...  1970\n",
       "2  36.\\tThe whole of progressive mankind recalls ...  1970\n",
       "3  37.\\tAll this has had well known consequences ...  1970\n",
       "4  38.\\tOne of the undeniable proofs that the Uni...  1970\n",
       "5  39.\\tUndoubtedly, such a state of affairs in t...  1970\n",
       "6  40.\\tThe liberation movement at the world leve...  1970\n",
       "7  41.\\tPanic-stricken at the impetuous growth of...  1970\n",
       "8  42.\\tAlthough split by numerous contradictions...  1970\n",
       "9  43.\\tIn that connexion we can cite, simultaneo...  1970"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_df = pd.DataFrame([{ \"text\": paragraph, \"year\": year }\n",
    "               for paragraphs, year in zip(df[\"paragraphs\"], df[\"year\"])\n",
    "                  for paragraph in paragraphs if paragraph])\n",
    "paragraph_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f8893",
   "metadata": {},
   "source": [
    "## Getting TFIDF of Sentences from DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d674bbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prave\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(279076, 25162)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "# Convert spaCy stopwords to a list\n",
    "stopwords_list = list(stopwords)\n",
    "\n",
    "# Create the TfidfVectorizer\n",
    "tfidf_para_vectorizer = TfidfVectorizer(stop_words=stopwords_list, min_df=5, max_df=0.7)\n",
    "tfidf_para_vectors = tfidf_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n",
    "\n",
    "# Display the shape of the resulting TF-IDF matrix\n",
    "tfidf_para_vectors.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e7279",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ddb05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_text_model = NMF(n_components=10, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccafedf",
   "metadata": {},
   "source": [
    "## Let’s Look at Our 10 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61746a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      " nuclear (1.04)\n",
      " weapons (0.55)\n",
      " disarmament (0.53)\n",
      " operation (0.39)\n",
      " treaty (0.36)\n",
      "\n",
      "Topic 01\n",
      " terrorism (0.36)\n",
      " challenges (0.31)\n",
      " cooperation (0.30)\n",
      " reform (0.29)\n",
      " sustainable (0.29)\n",
      "\n",
      "Topic 02\n",
      " africa (0.90)\n",
      " south (0.60)\n",
      " african (0.56)\n",
      " namibia (0.37)\n",
      " delegation (0.30)\n",
      "\n",
      "Topic 03\n",
      " arab (0.99)\n",
      " israel (0.86)\n",
      " palestinian (0.59)\n",
      " lebanon (0.54)\n",
      " israeli (0.53)\n",
      "\n",
      "Topic 04\n",
      " american (0.27)\n",
      " latin (0.27)\n",
      " america (0.27)\n",
      " panama (0.17)\n",
      " bolivia (0.16)\n",
      "\n",
      "Topic 05\n",
      " pacific (1.53)\n",
      " islands (1.25)\n",
      " island (0.87)\n",
      " solomon (0.86)\n",
      " fiji (0.70)\n",
      "\n",
      "Topic 06\n",
      " republic (0.84)\n",
      " viet (0.72)\n",
      " nam (0.67)\n",
      " socialist (0.55)\n",
      " soviet (0.54)\n",
      "\n",
      "Topic 07\n",
      " guinea (4.37)\n",
      " equatorial (1.79)\n",
      " bissau (1.55)\n",
      " papua (1.52)\n",
      " portugal (0.53)\n",
      "\n",
      "Topic 08\n",
      " european (0.81)\n",
      " europe (0.57)\n",
      " turkey (0.55)\n",
      " cyprus (0.51)\n",
      " greece (0.43)\n",
      "\n",
      "Topic 09\n",
      " caribbean (0.93)\n",
      " small (0.62)\n",
      " bahamas (0.60)\n",
      " saint (0.59)\n",
      " barbados (0.57)\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\" %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]] * 100.0 / total)))\n",
    "\n",
    "# Assuming you have already defined nmf_text_model and tfidf_text_vectorizer\n",
    "feature_names = tfidf_text_vectorizer.get_feature_names_out()\n",
    "display_topics(nmf_text_model, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f414fd",
   "metadata": {},
   "source": [
    "## Running Same Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9e52c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      " nations (5.61)\n",
      " united (5.50)\n",
      " organization (1.27)\n",
      " states (1.02)\n",
      " charter (0.93)\n",
      "\n",
      "Topic 01\n",
      " general (2.86)\n",
      " session (2.83)\n",
      " assembly (2.81)\n",
      " mr (1.99)\n",
      " president (1.81)\n",
      "\n",
      "Topic 02\n",
      " countries (4.41)\n",
      " developing (2.49)\n",
      " economic (1.50)\n",
      " developed (1.35)\n",
      " trade (0.92)\n",
      "\n",
      "Topic 03\n",
      " people (1.36)\n",
      " peace (1.33)\n",
      " east (1.29)\n",
      " middle (1.17)\n",
      " palestinian (1.16)\n",
      "\n",
      "Topic 04\n",
      " nuclear (4.93)\n",
      " weapons (3.26)\n",
      " disarmament (2.01)\n",
      " treaty (1.71)\n",
      " proliferation (1.46)\n",
      "\n",
      "Topic 05\n",
      " rights (6.48)\n",
      " human (6.16)\n",
      " respect (1.15)\n",
      " fundamental (0.85)\n",
      " universal (0.82)\n",
      "\n",
      "Topic 06\n",
      " africa (3.80)\n",
      " south (3.30)\n",
      " african (1.70)\n",
      " namibia (1.38)\n",
      " apartheid (1.18)\n",
      "\n",
      "Topic 07\n",
      " security (6.10)\n",
      " council (5.87)\n",
      " permanent (1.49)\n",
      " reform (1.49)\n",
      " peace (1.29)\n",
      "\n",
      "Topic 08\n",
      " international (2.03)\n",
      " world (1.48)\n",
      " community (0.91)\n",
      " new (0.76)\n",
      " peace (0.67)\n",
      "\n",
      "Topic 09\n",
      " development (4.47)\n",
      " sustainable (1.19)\n",
      " economic (1.06)\n",
      " social (0.99)\n",
      " goals (0.94)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Assuming tfidf_para_vectors and tfidf_para_vectorizer are already defined\n",
    "nmf_para_model = NMF(n_components=10, random_state=42)\n",
    "W_para_matrix = nmf_para_model.fit_transform(tfidf_para_vectors)\n",
    "H_para_matrix = nmf_para_model.components_\n",
    "display_topics(nmf_para_model, tfidf_para_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359fb56e",
   "metadata": {},
   "source": [
    "## How “BIG” is Each Topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7769838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.04128384, 16.66857283, 15.38093119, 10.36156837, 13.43233845,\n",
       "        5.9943591 ,  7.56900484,  4.23304994,  9.05044908,  7.26844235])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_text_matrix.sum(axis=0)/W_text_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca50400",
   "metadata": {},
   "source": [
    "## Topic sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec5d94a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.42905814, 10.3250673 , 10.19143368,  9.97207971,  6.63847604,\n",
       "        7.36011502,  8.92422143,  8.31107274, 16.87412537, 10.97435057])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_para_matrix.sum(axis=0)/W_para_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9284fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prave\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(279076, 25162)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert the set of stop words to a list\n",
    "stopwords_list = list(stopwords)\n",
    "\n",
    "count_para_vectorizer = CountVectorizer(stop_words=stopwords_list, min_df=5, max_df=0.7)\n",
    "count_para_vectors = count_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n",
    "count_para_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20d810",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fee444",
   "metadata": {},
   "source": [
    "## Starting by Cutting Paragraphs Into Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c07817a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279076, 25162)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the shape of the count vector matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert the set of stop words to a list\n",
    "stopwords_list = list(stopwords)\n",
    "\n",
    "count_para_vectorizer = CountVectorizer(stop_words=stopwords_list, min_df=5, max_df=0.7)\n",
    "count_para_vectors = count_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n",
    "count_para_vectors.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89569883",
   "metadata": {},
   "source": [
    "## Run the LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36755ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_para_model = LatentDirichletAllocation(n_components = 10,random_state=42)\n",
    "W_lda_para_matrix =lda_para_model.fit_transform(count_para_vectors)\n",
    "H_lda_para_matrix = lda_para_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81549fc",
   "metadata": {},
   "source": [
    "## Our 10 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88bbf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_lda_topics(model, feature_names, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1]\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\" %s (%2.2f)\" % (feature_names[largest[i]], abs(words[largest[i]] * 100.0 / total)))\n",
    "\n",
    "# Assuming you have already defined lda_para_model and count_para_vectorizer\n",
    "feature_names_lda = count_para_vectorizer.get_feature_names_out()\n",
    "display_lda_topics(lda_para_model, feature_names_lda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d3193f",
   "metadata": {},
   "source": [
    "## Topic size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93288700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage in the matrix W_lda_para_matrix\n",
    "W_lda_para_matrix.sum(axis=0)/W_lda_para_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4494a97",
   "metadata": {},
   "source": [
    "##  pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "\n",
    "# Assuming lda_para_model, count_para_vectors, and count_para_vectorizer are already defined\n",
    "\n",
    "# Extract vocabulary and term frequencies from CountVectorizer\n",
    "vocab = count_para_vectorizer.get_feature_names_out()\n",
    "term_frequency = count_para_vectors.sum(axis=0).A1\n",
    "\n",
    "# Get the topic-term distribution matrix from LDA model\n",
    "topic_term_dists = lda_para_model.components_\n",
    "\n",
    "# Get the document-topic distribution matrix from LDA model\n",
    "doc_topic_dists = lda_para_model.transform(count_para_vectors)\n",
    "\n",
    "# Get the document lengths\n",
    "doc_lengths = count_para_vectors.sum(axis=1).A1\n",
    "\n",
    "# Prepare the visualization\n",
    "lda_display = pyLDAvis.prepare(\n",
    "    topic_term_dists,\n",
    "    doc_topic_dists,\n",
    "    doc_lengths,\n",
    "    vocab=vocab,\n",
    "    term_frequency=term_frequency\n",
    ")\n",
    "\n",
    "# Display the visualization\n",
    "pyLDAvis.display(lda_display)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e98e1",
   "metadata": {},
   "source": [
    "# Drawing Word Clouds for 10 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1aa663",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>div.output_scroll { height: 44em; }</style>\"))\n",
    "\n",
    "def wordcloud_topics(model, features, no_top_words=40):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        size = {}\n",
    "        largest = words.argsort()[::-1]  # invert sort order\n",
    "        for i in range(0, no_top_words):\n",
    "            size[features[largest[i]]] = abs(words[largest[i]])\n",
    "\n",
    "        wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "        wc.generate_from_frequencies(size)\n",
    "\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # if you don't want to save the topic model, comment the next line\n",
    "        plt.savefig(f'topic{topic}.png')\n",
    "\n",
    "# Assuming you have already trained an NMF or LDA model and have the features (terms)\n",
    "# Replace the following with your actual model and features\n",
    "# wordcloud_topics(your_model, your_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_topics(nmf_para_model, count_para_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec58b9",
   "metadata": {},
   "source": [
    "## The LDA WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3649a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_topics(lda_para_model, count_para_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f1bd2e",
   "metadata": {},
   "source": [
    "## Topics Names by Joining Top 2 Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = []\n",
    "voc = count_para_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Vocabulary:\", voc)\n",
    "\n",
    "for topic in nmf_para_model.components_:\n",
    "    important = topic.argsort()\n",
    "    top_word = voc[important[-1]] + \" \" + voc[important[-2]]\n",
    "    topic_names.append(\"Topic \" + top_word)\n",
    "\n",
    "print(\"Topic Names:\", topic_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39f0d2",
   "metadata": {},
   "source": [
    "## Separating Data by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc835db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "year_data = []\n",
    "\n",
    "# Assuming paragraph_df is a DataFrame with a \"year\" column\n",
    "for year in tqdm(np.unique(paragraph_df[\"year\"])):\n",
    "    W_year = nmf_para_model.transform(tfidf_para_vectors[np.array(paragraph_df[\"year\"] == year)])\n",
    "    year_data.append([year] + list(W_year.sum(axis=0) / W_year.sum() * 100.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac89104",
   "metadata": {},
   "source": [
    "## Drawing Time Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_year = pd.DataFrame(year_data, columns=[\"year\"] +topic_names).set_index(\"year\")\n",
    "df_year.plot.area(figsize=(16,9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
